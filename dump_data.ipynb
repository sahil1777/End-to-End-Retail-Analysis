{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dumping from CSV Files to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "import os\n",
    "from sqlalchemy.sql import text\n",
    "import time\n",
    "\n",
    "# PostgreSQL connection details\n",
    "username = 'postgres'\n",
    "password = urllib.parse.quote_plus('enterpassword')  \n",
    "host = 'localhost'  \n",
    "port = '5432'  \n",
    "database = 'retail_db'\n",
    "\n",
    "# connection string\n",
    "connection_string = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "print(connection_string)\n",
    "\n",
    "# SQLAlchemy engine\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL DB successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine.connect()\n",
    "    print(\"Connection to PostgreSQL DB successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'INTEGER'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'FLOAT'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'BOOLEAN'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'TEXT'\n",
    "\n",
    "# List of CSV files and corresponding table names\n",
    "csv_files = [\n",
    "    ('customers.csv', 'customers'),\n",
    "    ('orders.csv', 'orders'),\n",
    "    ('sellers.csv', 'sellers'),\n",
    "    ('products.csv', 'products'),\n",
    "    ('geolocation.csv', 'geolocation'),\n",
    "    ('payments.csv', 'payments'),\n",
    "    ('order_items.csv', 'order_items')\n",
    "]\n",
    "\n",
    "\n",
    "folder_path = 'D:/END TO END RETAIL PROJECT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing customers.csv for table 'customers'\n",
      "Table 'customers' created or already exists.\n",
      "Chunk 1: 99441 rows inserted.\n",
      "Total rows inserted for customers: 99441\n",
      "Finished processing customers.csv in 14.01 seconds.\n",
      "\n",
      "Processing orders.csv for table 'orders'\n",
      "Table 'orders' created or already exists.\n",
      "Chunk 1: 99441 rows inserted.\n",
      "Total rows inserted for orders: 99441\n",
      "Finished processing orders.csv in 24.12 seconds.\n",
      "\n",
      "Processing sellers.csv for table 'sellers'\n",
      "Table 'sellers' created or already exists.\n",
      "Chunk 1: 3095 rows inserted.\n",
      "Total rows inserted for sellers: 3095\n",
      "Finished processing sellers.csv in 0.73 seconds.\n",
      "\n",
      "Processing products.csv for table 'products'\n",
      "Table 'products' created or already exists.\n",
      "Chunk 1: 32951 rows inserted.\n",
      "Total rows inserted for products: 32951\n",
      "Finished processing products.csv in 10.04 seconds.\n",
      "\n",
      "Processing geolocation.csv for table 'geolocation'\n",
      "Table 'geolocation' created or already exists.\n",
      "Chunk 1: 100000 rows inserted.\n",
      "Chunk 2: 100000 rows inserted.\n",
      "Chunk 3: 100000 rows inserted.\n",
      "Chunk 4: 100000 rows inserted.\n",
      "Chunk 5: 100000 rows inserted.\n",
      "Chunk 6: 100000 rows inserted.\n",
      "Chunk 7: 100000 rows inserted.\n",
      "Chunk 8: 100000 rows inserted.\n",
      "Chunk 9: 100000 rows inserted.\n",
      "Chunk 10: 100000 rows inserted.\n",
      "Chunk 11: 163 rows inserted.\n",
      "Total rows inserted for geolocation: 1000163\n",
      "Finished processing geolocation.csv in 126.67 seconds.\n",
      "\n",
      "Processing payments.csv for table 'payments'\n",
      "Table 'payments' created or already exists.\n",
      "Chunk 1: 100000 rows inserted.\n",
      "Chunk 2: 3886 rows inserted.\n",
      "Total rows inserted for payments: 103886\n",
      "Finished processing payments.csv in 11.87 seconds.\n",
      "\n",
      "Processing order_items.csv for table 'order_items'\n",
      "Table 'order_items' created or already exists.\n",
      "Chunk 1: 100000 rows inserted.\n",
      "Chunk 2: 12650 rows inserted.\n",
      "Total rows inserted for order_items: 112650\n",
      "Finished processing order_items.csv in 21.03 seconds.\n",
      "CSV files successfully uploaded to the PostgreSQL database!\n"
     ]
    }
   ],
   "source": [
    "for csv_file, table_name in csv_files:\n",
    "    start_time = time.time()  \n",
    "    \n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    print(f\"\\nProcessing {csv_file} for table '{table_name}'\")\n",
    "\n",
    "    chunk_size = 100000  \n",
    "    total_rows = 0  \n",
    "\n",
    "    for chunk_num, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "      \n",
    "        chunk = chunk.where(pd.notnull(chunk), None)\n",
    "        \n",
    "        # Clean column names\n",
    "        chunk.columns = [col.replace(' ', '_').replace('-', '_').replace('.', '_') for col in chunk.columns]\n",
    "        \n",
    "        if chunk_num == 0:\n",
    "           \n",
    "            columns = ', '.join([f'\"{col}\" {get_sql_type(chunk[col].dtype)}' for col in chunk.columns])\n",
    "            create_table_query = f'CREATE TABLE IF NOT EXISTS \"{table_name}\" ({columns});'\n",
    "            \n",
    "            with engine.begin() as connection:\n",
    "                connection.execute(text(create_table_query))\n",
    "                print(f\"Table '{table_name}' created or already exists.\")\n",
    "\n",
    " \n",
    "        chunk.to_sql(table_name, engine, if_exists='append', index=False, method='multi')\n",
    "        \n",
    "        rows_inserted = len(chunk)\n",
    "        total_rows += rows_inserted\n",
    "        print(f\"Chunk {chunk_num + 1}: {rows_inserted} rows inserted.\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print(f\"Total rows inserted for {table_name}: {total_rows}\")\n",
    "    print(f\"Finished processing {csv_file} in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "print(\"CSV files successfully uploaded to the PostgreSQL database!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
